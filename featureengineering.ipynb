{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "The goals of this course are to:\n",
    "\n",
    "-   Understand the importance of feature engineering\n",
    "-   Learn how to create new features from existing data\n",
    "-   Learn how to use domain knowledge to create new features\n",
    "-   Learn how to encode categorical variables for machine learning\n",
    "-   Learn how to handle missing values in a dataset\n",
    "-   Learn how to deal with time series data\n",
    "-   Learn how to scale features for machine learning\n",
    "-   Learn how to evaluate features\n",
    "-   Learn how to use Pandas and Scikit-Learn to engineer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "*Imputation* means to fill in missing values with plausible values. There are many\n",
    "options:\n",
    "\n",
    "-  A constant value that has meaning within the domain, such as 0, distinct from all\n",
    "    other values.\n",
    "-  A value from another randomly selected record.\n",
    "-  A mean, median or mode value for the column.\n",
    "-  A value estimated by another predictive model.\n",
    "\n",
    "We use imputation because many machine learning algorithms do not support missing values. Modern algorithms like XGBoost handle missing values themselves, but it is still a common practice to impute because other algorithms do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Fuel economy data from the U.S. Environmental Protection Agency (EPA) for 2019 model year vehicles. The data are available in a CSV file with 82,000 rows and 83 columns.\n",
    "\n",
    "https://www.fueleconomy.gov/feg/epadata/vehicles.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /home/codespace/.local/lib/python3.10/site-packages (15.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in /home/codespace/.local/lib/python3.10/site-packages (from pyarrow) (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3134/794973991.py:4: DtypeWarning: Columns (72,74,75,77) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw = pd.read_csv(url)#, dtype_backend='pyarrow', engine='pyarrow')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = 'https://www.fueleconomy.gov/feg/epadata/vehicles.csv' \n",
    "\n",
    "raw = pd.read_csv(url)#, dtype_backend='pyarrow', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>barrels08</th>\n",
       "      <th>barrelsA08</th>\n",
       "      <th>charge120</th>\n",
       "      <th>charge240</th>\n",
       "      <th>city08</th>\n",
       "      <th>city08U</th>\n",
       "      <th>cityA08</th>\n",
       "      <th>cityA08U</th>\n",
       "      <th>cityCD</th>\n",
       "      <th>cityE</th>\n",
       "      <th>...</th>\n",
       "      <th>mfrCode</th>\n",
       "      <th>c240Dscr</th>\n",
       "      <th>charge240b</th>\n",
       "      <th>c240bDscr</th>\n",
       "      <th>createdOn</th>\n",
       "      <th>modifiedOn</th>\n",
       "      <th>startStop</th>\n",
       "      <th>phevCity</th>\n",
       "      <th>phevHwy</th>\n",
       "      <th>phevComb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.167143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.046364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.018889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.046364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.658421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47518</th>\n",
       "      <td>13.523182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47519</th>\n",
       "      <td>12.935217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47520</th>\n",
       "      <td>14.167143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47521</th>\n",
       "      <td>14.167143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47522</th>\n",
       "      <td>16.528333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>Tue Jan 01 00:00:00 EST 2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47523 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       barrels08  barrelsA08  charge120  charge240  city08  city08U  cityA08  \\\n",
       "0      14.167143         0.0        0.0        0.0      19      0.0        0   \n",
       "1      27.046364         0.0        0.0        0.0       9      0.0        0   \n",
       "2      11.018889         0.0        0.0        0.0      23      0.0        0   \n",
       "3      27.046364         0.0        0.0        0.0      10      0.0        0   \n",
       "4      15.658421         0.0        0.0        0.0      17      0.0        0   \n",
       "...          ...         ...        ...        ...     ...      ...      ...   \n",
       "47518  13.523182         0.0        0.0        0.0      19      0.0        0   \n",
       "47519  12.935217         0.0        0.0        0.0      20      0.0        0   \n",
       "47520  14.167143         0.0        0.0        0.0      18      0.0        0   \n",
       "47521  14.167143         0.0        0.0        0.0      18      0.0        0   \n",
       "47522  16.528333         0.0        0.0        0.0      16      0.0        0   \n",
       "\n",
       "       cityA08U  cityCD  cityE  ...  mfrCode  c240Dscr  charge240b  c240bDscr  \\\n",
       "0           0.0     0.0    0.0  ...      NaN       NaN         0.0        NaN   \n",
       "1           0.0     0.0    0.0  ...      NaN       NaN         0.0        NaN   \n",
       "2           0.0     0.0    0.0  ...      NaN       NaN         0.0        NaN   \n",
       "3           0.0     0.0    0.0  ...      NaN       NaN         0.0        NaN   \n",
       "4           0.0     0.0    0.0  ...      NaN       NaN         0.0        NaN   \n",
       "...         ...     ...    ...  ...      ...       ...         ...        ...   \n",
       "47518       0.0     0.0    0.0  ...      NaN       NaN         0.0        NaN   \n",
       "47519       0.0     0.0    0.0  ...      NaN       NaN         0.0        NaN   \n",
       "47520       0.0     0.0    0.0  ...      NaN       NaN         0.0        NaN   \n",
       "47521       0.0     0.0    0.0  ...      NaN       NaN         0.0        NaN   \n",
       "47522       0.0     0.0    0.0  ...      NaN       NaN         0.0        NaN   \n",
       "\n",
       "                          createdOn                    modifiedOn  startStop  \\\n",
       "0      Tue Jan 01 00:00:00 EST 2013  Tue Jan 01 00:00:00 EST 2013        NaN   \n",
       "1      Tue Jan 01 00:00:00 EST 2013  Tue Jan 01 00:00:00 EST 2013        NaN   \n",
       "2      Tue Jan 01 00:00:00 EST 2013  Tue Jan 01 00:00:00 EST 2013        NaN   \n",
       "3      Tue Jan 01 00:00:00 EST 2013  Tue Jan 01 00:00:00 EST 2013        NaN   \n",
       "4      Tue Jan 01 00:00:00 EST 2013  Tue Jan 01 00:00:00 EST 2013        NaN   \n",
       "...                             ...                           ...        ...   \n",
       "47518  Tue Jan 01 00:00:00 EST 2013  Tue Jan 01 00:00:00 EST 2013        NaN   \n",
       "47519  Tue Jan 01 00:00:00 EST 2013  Tue Jan 01 00:00:00 EST 2013        NaN   \n",
       "47520  Tue Jan 01 00:00:00 EST 2013  Tue Jan 01 00:00:00 EST 2013        NaN   \n",
       "47521  Tue Jan 01 00:00:00 EST 2013  Tue Jan 01 00:00:00 EST 2013        NaN   \n",
       "47522  Tue Jan 01 00:00:00 EST 2013  Tue Jan 01 00:00:00 EST 2013        NaN   \n",
       "\n",
       "       phevCity  phevHwy  phevComb  \n",
       "0             0        0         0  \n",
       "1             0        0         0  \n",
       "2             0        0         0  \n",
       "3             0        0         0  \n",
       "4             0        0         0  \n",
       "...         ...      ...       ...  \n",
       "47518         0        0         0  \n",
       "47519         0        0         0  \n",
       "47520         0        0         0  \n",
       "47521         0        0         0  \n",
       "47522         0        0         0  \n",
       "\n",
       "[47523 rows x 84 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['barrels08', 'barrelsA08', 'charge120', 'charge240', 'city08',\n",
       "       'city08U', 'cityA08', 'cityA08U', 'cityCD', 'cityE', 'cityUF', 'co2',\n",
       "       'co2A', 'co2TailpipeAGpm', 'co2TailpipeGpm', 'comb08', 'comb08U',\n",
       "       'combA08', 'combA08U', 'combE', 'combinedCD', 'combinedUF', 'cylinders',\n",
       "       'displ', 'drive', 'engId', 'eng_dscr', 'feScore', 'fuelCost08',\n",
       "       'fuelCostA08', 'fuelType', 'fuelType1', 'ghgScore', 'ghgScoreA',\n",
       "       'highway08', 'highway08U', 'highwayA08', 'highwayA08U', 'highwayCD',\n",
       "       'highwayE', 'highwayUF', 'hlv', 'hpv', 'id', 'lv2', 'lv4', 'make',\n",
       "       'model', 'mpgData', 'phevBlended', 'pv2', 'pv4', 'range', 'rangeCity',\n",
       "       'rangeCityA', 'rangeHwy', 'rangeHwyA', 'trany', 'UCity', 'UCityA',\n",
       "       'UHighway', 'UHighwayA', 'VClass', 'year', 'youSaveSpend', 'baseModel',\n",
       "       'guzzler', 'trans_dscr', 'tCharger', 'sCharger', 'atvType', 'fuelType2',\n",
       "       'rangeA', 'evMotor', 'mfrCode', 'c240Dscr', 'charge240b', 'c240bDscr',\n",
       "       'createdOn', 'modifiedOn', 'startStop', 'phevCity', 'phevHwy',\n",
       "       'phevComb'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Tue Jan 01 00:00:00 EST 2013\n",
      "1        Tue Jan 01 00:00:00 EST 2013\n",
      "2        Tue Jan 01 00:00:00 EST 2013\n",
      "3        Tue Jan 01 00:00:00 EST 2013\n",
      "4        Tue Jan 01 00:00:00 EST 2013\n",
      "                     ...             \n",
      "47518    Tue Jan 01 00:00:00 EST 2013\n",
      "47519    Tue Jan 01 00:00:00 EST 2013\n",
      "47520    Tue Jan 01 00:00:00 EST 2013\n",
      "47521    Tue Jan 01 00:00:00 EST 2013\n",
      "47522    Tue Jan 01 00:00:00 EST 2013\n",
      "Name: createdOn, Length: 47523, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(raw.createdOn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      offset\n",
      "0        EST\n",
      "1        EST\n",
      "2        EST\n",
      "3        EST\n",
      "4        EST\n",
      "...      ...\n",
      "47518    EST\n",
      "47519    EST\n",
      "47520    EST\n",
      "47521    EST\n",
      "47522    EST\n",
      "\n",
      "[47523 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(raw.createdOn.str.extract(r'\\d\\d:\\d\\d (?P<offset>[A-Z]{3}?)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                                     int64\n",
       "make                                    object\n",
       "model                                   object\n",
       "trany                                   object\n",
       "drive                                   object\n",
       "VClass                                  object\n",
       "eng_dscr                                object\n",
       "barrels08                              float64\n",
       "city08                                   int64\n",
       "comb08                                   int64\n",
       "range                                    int64\n",
       "evMotor                                 object\n",
       "cylinders                              float64\n",
       "displ                                  float64\n",
       "fuelCost08                               int64\n",
       "fuelType                                object\n",
       "highway08                                int64\n",
       "trans_dscr                              object\n",
       "createdOn     datetime64[ns, America/New_York]\n",
       "offset                                  object\n",
       "str_date                                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['year', 'make', 'model', 'trany', 'drive', 'VClass', 'eng_dscr',\n",
    "    'barrels08', 'city08', 'comb08', 'range', 'evMotor', 'cylinders', 'displ', 'fuelCost08', \n",
    "        'fuelType', 'highway08',  'trans_dscr','createdOn']\n",
    "\n",
    "def to_tz(df_, time_col, tz_offset, tz_name):\n",
    "    return (df_\n",
    "            .groupby(tz_offset)\n",
    "            [time_col]\n",
    "            .transform(lambda s: pd.to_datetime(s)\n",
    "                       .dt.tz_localize(s.name, ambiguous=True)\n",
    "                       .dt.tz_convert(tz_name))\n",
    "            )\n",
    "\n",
    "autos = (raw.loc[:, cols]\n",
    "         .assign(\n",
    "            offset=(raw.createdOn.str.extract(r'\\d\\d:\\d\\d (?P<offset>[A-Z]{3}?)')\n",
    "                .replace('EDT', 'EST5EDT')),\n",
    "            str_date=(raw.createdOn.str.slice(4,19) + ' ' +\n",
    "                raw.createdOn.str.slice(-4)),\n",
    "            createdOn=lambda df_: to_tz(df_, 'str_date', 'offset', 'America/New_York')\n",
    "         )\n",
    ")\n",
    "autos.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>trany</th>\n",
       "      <th>drive</th>\n",
       "      <th>VClass</th>\n",
       "      <th>eng_dscr</th>\n",
       "      <th>barrels08</th>\n",
       "      <th>city08</th>\n",
       "      <th>comb08</th>\n",
       "      <th>...</th>\n",
       "      <th>evMotor</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displ</th>\n",
       "      <th>fuelCost08</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>highway08</th>\n",
       "      <th>trans_dscr</th>\n",
       "      <th>createdOn</th>\n",
       "      <th>offset</th>\n",
       "      <th>str_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1985</td>\n",
       "      <td>Alfa Romeo</td>\n",
       "      <td>Spider Veloce 2000</td>\n",
       "      <td>Manual 5-spd</td>\n",
       "      <td>Rear-Wheel Drive</td>\n",
       "      <td>Two Seaters</td>\n",
       "      <td>(FFS)</td>\n",
       "      <td>14.167143</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2450</td>\n",
       "      <td>Regular</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1985</td>\n",
       "      <td>Ferrari</td>\n",
       "      <td>Testarossa</td>\n",
       "      <td>Manual 5-spd</td>\n",
       "      <td>Rear-Wheel Drive</td>\n",
       "      <td>Two Seaters</td>\n",
       "      <td>(GUZZLER)</td>\n",
       "      <td>27.046364</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4700</td>\n",
       "      <td>Regular</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1985</td>\n",
       "      <td>Dodge</td>\n",
       "      <td>Charger</td>\n",
       "      <td>Manual 5-spd</td>\n",
       "      <td>Front-Wheel Drive</td>\n",
       "      <td>Subcompact Cars</td>\n",
       "      <td>(FFS)</td>\n",
       "      <td>11.018889</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1900</td>\n",
       "      <td>Regular</td>\n",
       "      <td>33</td>\n",
       "      <td>SIL</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year        make               model         trany              drive  \\\n",
       "0  1985  Alfa Romeo  Spider Veloce 2000  Manual 5-spd   Rear-Wheel Drive   \n",
       "1  1985     Ferrari          Testarossa  Manual 5-spd   Rear-Wheel Drive   \n",
       "2  1985       Dodge             Charger  Manual 5-spd  Front-Wheel Drive   \n",
       "\n",
       "            VClass   eng_dscr  barrels08  city08  comb08  ...  evMotor  \\\n",
       "0      Two Seaters      (FFS)  14.167143      19      21  ...      NaN   \n",
       "1      Two Seaters  (GUZZLER)  27.046364       9      11  ...      NaN   \n",
       "2  Subcompact Cars      (FFS)  11.018889      23      27  ...      NaN   \n",
       "\n",
       "  cylinders  displ  fuelCost08  fuelType highway08  trans_dscr  \\\n",
       "0       4.0    2.0        2450   Regular        25         NaN   \n",
       "1      12.0    4.9        4700   Regular        14         NaN   \n",
       "2       4.0    2.2        1900   Regular        33         SIL   \n",
       "\n",
       "                  createdOn offset              str_date  \n",
       "0 2013-01-01 00:00:00-05:00    EST  Jan 01 00:00:00 2013  \n",
       "1 2013-01-01 00:00:00-05:00    EST  Jan 01 00:00:00 2013  \n",
       "2 2013-01-01 00:00:00-05:00    EST  Jan 01 00:00:00 2013  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autos.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key insight in Python\n",
    "# booleans are integers\n",
    "\n",
    "True + 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "False + 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year           0.0\n",
       "make           0.0\n",
       "model          0.0\n",
       "trany          0.0\n",
       "drive          2.5\n",
       "VClass         0.0\n",
       "eng_dscr      36.7\n",
       "barrels08      0.0\n",
       "city08         0.0\n",
       "comb08         0.0\n",
       "range          0.0\n",
       "evMotor       94.9\n",
       "cylinders      1.7\n",
       "displ          1.7\n",
       "fuelCost08     0.0\n",
       "fuelType       0.0\n",
       "highway08      0.0\n",
       "trans_dscr    68.3\n",
       "createdOn      0.0\n",
       "offset         0.0\n",
       "str_date       0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determining missing values\n",
    "(autos\n",
    " .isna()\n",
    " #.sum()\n",
    " .mean().mul(100).round(1)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drive          1186\n",
       "eng_dscr      17442\n",
       "fuelType          0\n",
       "fuelType1         0\n",
       "make              0\n",
       "model             0\n",
       "mpgData           0\n",
       "trany            11\n",
       "VClass            0\n",
       "baseModel         0\n",
       "guzzler       44760\n",
       "trans_dscr    32479\n",
       "tCharger      37188\n",
       "sCharger      46517\n",
       "atvType       42202\n",
       "fuelType2     45627\n",
       "rangeA        45632\n",
       "evMotor       45086\n",
       "mfrCode       30808\n",
       "c240Dscr      47382\n",
       "c240bDscr     47388\n",
       "createdOn         0\n",
       "modifiedOn        0\n",
       "startStop     31689\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw\n",
    " #.select_dtypes('string[pyarrow]')\n",
    " .select_dtypes(object)\n",
    " .isna()\n",
    " .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in Missing Values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>trany</th>\n",
       "      <th>drive</th>\n",
       "      <th>VClass</th>\n",
       "      <th>eng_dscr</th>\n",
       "      <th>barrels08</th>\n",
       "      <th>city08</th>\n",
       "      <th>comb08</th>\n",
       "      <th>...</th>\n",
       "      <th>evMotor</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displ</th>\n",
       "      <th>fuelCost08</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>highway08</th>\n",
       "      <th>trans_dscr</th>\n",
       "      <th>createdOn</th>\n",
       "      <th>offset</th>\n",
       "      <th>str_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7138</th>\n",
       "      <td>2000</td>\n",
       "      <td>Nissan</td>\n",
       "      <td>Altra EV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Midsize Station Wagons</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0960</td>\n",
       "      <td>81</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>62 KW AC Induction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>900</td>\n",
       "      <td>Electricity</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7139</th>\n",
       "      <td>2000</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>RAV4 EV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2-Wheel Drive</td>\n",
       "      <td>Sport Utility Vehicle - 2WD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1128</td>\n",
       "      <td>81</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>50 KW DC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1050</td>\n",
       "      <td>Electricity</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8143</th>\n",
       "      <td>2001</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>RAV4 EV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2-Wheel Drive</td>\n",
       "      <td>Sport Utility Vehicle - 2WD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1128</td>\n",
       "      <td>81</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>50 KW DC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1050</td>\n",
       "      <td>Electricity</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8144</th>\n",
       "      <td>2001</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Th!nk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two Seaters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>74</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>27 KW AC Induction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1150</td>\n",
       "      <td>Electricity</td>\n",
       "      <td>58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8146</th>\n",
       "      <td>2001</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Explorer USPS Electric</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2-Wheel Drive</td>\n",
       "      <td>Sport Utility Vehicle - 2WD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2088</td>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>67 KW AC Induction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1950</td>\n",
       "      <td>Electricity</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41693</th>\n",
       "      <td>2024</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>bZ4X AWD</td>\n",
       "      <td>Automatic (A1)</td>\n",
       "      <td>All-Wheel Drive</td>\n",
       "      <td>Small Sport Utility Vehicle 4WD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>114</td>\n",
       "      <td>104</td>\n",
       "      <td>...</td>\n",
       "      <td>80 and 80 kW AC Synchronous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700</td>\n",
       "      <td>Electricity</td>\n",
       "      <td>94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-02 00:00:00-04:00</td>\n",
       "      <td>EST5EDT</td>\n",
       "      <td>May 02 00:00:00 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41694</th>\n",
       "      <td>2024</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>bZ4X Limited AWD</td>\n",
       "      <td>Automatic (A1)</td>\n",
       "      <td>All-Wheel Drive</td>\n",
       "      <td>Small Sport Utility Vehicle 4WD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>112</td>\n",
       "      <td>102</td>\n",
       "      <td>...</td>\n",
       "      <td>80 and 80 kW AC Synchronous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700</td>\n",
       "      <td>Electricity</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-02 00:00:00-04:00</td>\n",
       "      <td>EST5EDT</td>\n",
       "      <td>May 02 00:00:00 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41695</th>\n",
       "      <td>2024</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>ID.4</td>\n",
       "      <td>Automatic (A1)</td>\n",
       "      <td>Rear-Wheel Drive</td>\n",
       "      <td>Small Sport Utility Vehicle 2WD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>115</td>\n",
       "      <td>107</td>\n",
       "      <td>...</td>\n",
       "      <td>210 kW AC 3-Phase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700</td>\n",
       "      <td>Electricity</td>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-02 00:00:00-04:00</td>\n",
       "      <td>EST5EDT</td>\n",
       "      <td>May 02 00:00:00 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41696</th>\n",
       "      <td>2024</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>ID.4 Pro S</td>\n",
       "      <td>Automatic (A1)</td>\n",
       "      <td>Rear-Wheel Drive</td>\n",
       "      <td>Small Sport Utility Vehicle 2WD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0720</td>\n",
       "      <td>122</td>\n",
       "      <td>113</td>\n",
       "      <td>...</td>\n",
       "      <td>210 kW AC 3-Phase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>650</td>\n",
       "      <td>Electricity</td>\n",
       "      <td>104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-02 00:00:00-04:00</td>\n",
       "      <td>EST5EDT</td>\n",
       "      <td>May 02 00:00:00 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697</th>\n",
       "      <td>2024</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>ID.4 S</td>\n",
       "      <td>Automatic (A1)</td>\n",
       "      <td>Rear-Wheel Drive</td>\n",
       "      <td>Small Sport Utility Vehicle 2WD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>115</td>\n",
       "      <td>107</td>\n",
       "      <td>...</td>\n",
       "      <td>210 kW AC 3-Phase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700</td>\n",
       "      <td>Electricity</td>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-02 00:00:00-04:00</td>\n",
       "      <td>EST5EDT</td>\n",
       "      <td>May 02 00:00:00 2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>801 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year        make                   model           trany  \\\n",
       "7138   2000      Nissan                Altra EV             NaN   \n",
       "7139   2000      Toyota                 RAV4 EV             NaN   \n",
       "8143   2001      Toyota                 RAV4 EV             NaN   \n",
       "8144   2001        Ford                   Th!nk             NaN   \n",
       "8146   2001        Ford  Explorer USPS Electric             NaN   \n",
       "...     ...         ...                     ...             ...   \n",
       "41693  2024      Toyota                bZ4X AWD  Automatic (A1)   \n",
       "41694  2024      Toyota        bZ4X Limited AWD  Automatic (A1)   \n",
       "41695  2024  Volkswagen                    ID.4  Automatic (A1)   \n",
       "41696  2024  Volkswagen              ID.4 Pro S  Automatic (A1)   \n",
       "41697  2024  Volkswagen                  ID.4 S  Automatic (A1)   \n",
       "\n",
       "                  drive                           VClass eng_dscr  barrels08  \\\n",
       "7138                NaN           Midsize Station Wagons      NaN     0.0960   \n",
       "7139      2-Wheel Drive      Sport Utility Vehicle - 2WD      NaN     0.1128   \n",
       "8143      2-Wheel Drive      Sport Utility Vehicle - 2WD      NaN     0.1128   \n",
       "8144                NaN                      Two Seaters      NaN     0.1248   \n",
       "8146      2-Wheel Drive      Sport Utility Vehicle - 2WD      NaN     0.2088   \n",
       "...                 ...                              ...      ...        ...   \n",
       "41693   All-Wheel Drive  Small Sport Utility Vehicle 4WD      NaN     0.0744   \n",
       "41694   All-Wheel Drive  Small Sport Utility Vehicle 4WD      NaN     0.0768   \n",
       "41695  Rear-Wheel Drive  Small Sport Utility Vehicle 2WD      NaN     0.0768   \n",
       "41696  Rear-Wheel Drive  Small Sport Utility Vehicle 2WD      NaN     0.0720   \n",
       "41697  Rear-Wheel Drive  Small Sport Utility Vehicle 2WD      NaN     0.0768   \n",
       "\n",
       "       city08  comb08  ...                      evMotor cylinders  displ  \\\n",
       "7138       81      85  ...           62 KW AC Induction       NaN    NaN   \n",
       "7139       81      72  ...                     50 KW DC       NaN    NaN   \n",
       "8143       81      72  ...                     50 KW DC       NaN    NaN   \n",
       "8144       74      65  ...           27 KW AC Induction       NaN    NaN   \n",
       "8146       45      39  ...           67 KW AC Induction       NaN    NaN   \n",
       "...       ...     ...  ...                          ...       ...    ...   \n",
       "41693     114     104  ...  80 and 80 kW AC Synchronous       NaN    NaN   \n",
       "41694     112     102  ...  80 and 80 kW AC Synchronous       NaN    NaN   \n",
       "41695     115     107  ...            210 kW AC 3-Phase       NaN    NaN   \n",
       "41696     122     113  ...            210 kW AC 3-Phase       NaN    NaN   \n",
       "41697     115     107  ...            210 kW AC 3-Phase       NaN    NaN   \n",
       "\n",
       "       fuelCost08     fuelType highway08  trans_dscr  \\\n",
       "7138          900  Electricity        91         NaN   \n",
       "7139         1050  Electricity        64         NaN   \n",
       "8143         1050  Electricity        64         NaN   \n",
       "8144         1150  Electricity        58         NaN   \n",
       "8146         1950  Electricity        33         NaN   \n",
       "...           ...          ...       ...         ...   \n",
       "41693         700  Electricity        94         NaN   \n",
       "41694         700  Electricity        92         NaN   \n",
       "41695         700  Electricity        98         NaN   \n",
       "41696         650  Electricity       104         NaN   \n",
       "41697         700  Electricity        98         NaN   \n",
       "\n",
       "                      createdOn   offset              str_date  \n",
       "7138  2013-01-01 00:00:00-05:00      EST  Jan 01 00:00:00 2013  \n",
       "7139  2013-01-01 00:00:00-05:00      EST  Jan 01 00:00:00 2013  \n",
       "8143  2013-01-01 00:00:00-05:00      EST  Jan 01 00:00:00 2013  \n",
       "8144  2013-01-01 00:00:00-05:00      EST  Jan 01 00:00:00 2013  \n",
       "8146  2013-01-01 00:00:00-05:00      EST  Jan 01 00:00:00 2013  \n",
       "...                         ...      ...                   ...  \n",
       "41693 2024-05-02 00:00:00-04:00  EST5EDT  May 02 00:00:00 2024  \n",
       "41694 2024-05-02 00:00:00-04:00  EST5EDT  May 02 00:00:00 2024  \n",
       "41695 2024-05-02 00:00:00-04:00  EST5EDT  May 02 00:00:00 2024  \n",
       "41696 2024-05-02 00:00:00-04:00  EST5EDT  May 02 00:00:00 2024  \n",
       "41697 2024-05-02 00:00:00-04:00  EST5EDT  May 02 00:00:00 2024  \n",
       "\n",
       "[801 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where are cylinders missing?\n",
    "\n",
    "(autos\n",
    " .query('cylinders.isna()')\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>trany</th>\n",
       "      <th>drive</th>\n",
       "      <th>VClass</th>\n",
       "      <th>eng_dscr</th>\n",
       "      <th>barrels08</th>\n",
       "      <th>city08</th>\n",
       "      <th>comb08</th>\n",
       "      <th>...</th>\n",
       "      <th>evMotor</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displ</th>\n",
       "      <th>fuelCost08</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>highway08</th>\n",
       "      <th>trans_dscr</th>\n",
       "      <th>createdOn</th>\n",
       "      <th>offset</th>\n",
       "      <th>str_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1985</td>\n",
       "      <td>Alfa Romeo</td>\n",
       "      <td>Spider Veloce 2000</td>\n",
       "      <td>Manual 5-spd</td>\n",
       "      <td>Rear-Wheel Drive</td>\n",
       "      <td>Two Seaters</td>\n",
       "      <td>(FFS)</td>\n",
       "      <td>14.167143</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2450</td>\n",
       "      <td>Regular</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1985</td>\n",
       "      <td>Ferrari</td>\n",
       "      <td>Testarossa</td>\n",
       "      <td>Manual 5-spd</td>\n",
       "      <td>Rear-Wheel Drive</td>\n",
       "      <td>Two Seaters</td>\n",
       "      <td>(GUZZLER)</td>\n",
       "      <td>27.046364</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4700</td>\n",
       "      <td>Regular</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1985</td>\n",
       "      <td>Dodge</td>\n",
       "      <td>Charger</td>\n",
       "      <td>Manual 5-spd</td>\n",
       "      <td>Front-Wheel Drive</td>\n",
       "      <td>Subcompact Cars</td>\n",
       "      <td>(FFS)</td>\n",
       "      <td>11.018889</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1900</td>\n",
       "      <td>Regular</td>\n",
       "      <td>33</td>\n",
       "      <td>SIL</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1985</td>\n",
       "      <td>Dodge</td>\n",
       "      <td>B150/B250 Wagon 2WD</td>\n",
       "      <td>Automatic 3-spd</td>\n",
       "      <td>Rear-Wheel Drive</td>\n",
       "      <td>Vans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.046364</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4700</td>\n",
       "      <td>Regular</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1993</td>\n",
       "      <td>Subaru</td>\n",
       "      <td>Legacy AWD Turbo</td>\n",
       "      <td>Manual 5-spd</td>\n",
       "      <td>4-Wheel or All-Wheel Drive</td>\n",
       "      <td>Compact Cars</td>\n",
       "      <td>(FFS,TRBO)</td>\n",
       "      <td>15.658421</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3400</td>\n",
       "      <td>Premium</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47518</th>\n",
       "      <td>1993</td>\n",
       "      <td>Subaru</td>\n",
       "      <td>Legacy</td>\n",
       "      <td>Automatic 4-spd</td>\n",
       "      <td>Front-Wheel Drive</td>\n",
       "      <td>Compact Cars</td>\n",
       "      <td>(FFS)</td>\n",
       "      <td>13.523182</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2350</td>\n",
       "      <td>Regular</td>\n",
       "      <td>26</td>\n",
       "      <td>CLKUP</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47519</th>\n",
       "      <td>1993</td>\n",
       "      <td>Subaru</td>\n",
       "      <td>Legacy</td>\n",
       "      <td>Manual 5-spd</td>\n",
       "      <td>Front-Wheel Drive</td>\n",
       "      <td>Compact Cars</td>\n",
       "      <td>(FFS)</td>\n",
       "      <td>12.935217</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2250</td>\n",
       "      <td>Regular</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47520</th>\n",
       "      <td>1993</td>\n",
       "      <td>Subaru</td>\n",
       "      <td>Legacy AWD</td>\n",
       "      <td>Automatic 4-spd</td>\n",
       "      <td>4-Wheel or All-Wheel Drive</td>\n",
       "      <td>Compact Cars</td>\n",
       "      <td>(FFS)</td>\n",
       "      <td>14.167143</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2450</td>\n",
       "      <td>Regular</td>\n",
       "      <td>24</td>\n",
       "      <td>CLKUP</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47521</th>\n",
       "      <td>1993</td>\n",
       "      <td>Subaru</td>\n",
       "      <td>Legacy AWD</td>\n",
       "      <td>Manual 5-spd</td>\n",
       "      <td>4-Wheel or All-Wheel Drive</td>\n",
       "      <td>Compact Cars</td>\n",
       "      <td>(FFS)</td>\n",
       "      <td>14.167143</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2450</td>\n",
       "      <td>Regular</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47522</th>\n",
       "      <td>1993</td>\n",
       "      <td>Subaru</td>\n",
       "      <td>Legacy AWD Turbo</td>\n",
       "      <td>Automatic 4-spd</td>\n",
       "      <td>4-Wheel or All-Wheel Drive</td>\n",
       "      <td>Compact Cars</td>\n",
       "      <td>(FFS,TRBO)</td>\n",
       "      <td>16.528333</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3600</td>\n",
       "      <td>Premium</td>\n",
       "      <td>21</td>\n",
       "      <td>CLKUP</td>\n",
       "      <td>2013-01-01 00:00:00-05:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>Jan 01 00:00:00 2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47523 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year        make                model            trany  \\\n",
       "0      1985  Alfa Romeo   Spider Veloce 2000     Manual 5-spd   \n",
       "1      1985     Ferrari           Testarossa     Manual 5-spd   \n",
       "2      1985       Dodge              Charger     Manual 5-spd   \n",
       "3      1985       Dodge  B150/B250 Wagon 2WD  Automatic 3-spd   \n",
       "4      1993      Subaru     Legacy AWD Turbo     Manual 5-spd   \n",
       "...     ...         ...                  ...              ...   \n",
       "47518  1993      Subaru               Legacy  Automatic 4-spd   \n",
       "47519  1993      Subaru               Legacy     Manual 5-spd   \n",
       "47520  1993      Subaru           Legacy AWD  Automatic 4-spd   \n",
       "47521  1993      Subaru           Legacy AWD     Manual 5-spd   \n",
       "47522  1993      Subaru     Legacy AWD Turbo  Automatic 4-spd   \n",
       "\n",
       "                            drive           VClass    eng_dscr  barrels08  \\\n",
       "0                Rear-Wheel Drive      Two Seaters       (FFS)  14.167143   \n",
       "1                Rear-Wheel Drive      Two Seaters   (GUZZLER)  27.046364   \n",
       "2               Front-Wheel Drive  Subcompact Cars       (FFS)  11.018889   \n",
       "3                Rear-Wheel Drive             Vans         NaN  27.046364   \n",
       "4      4-Wheel or All-Wheel Drive     Compact Cars  (FFS,TRBO)  15.658421   \n",
       "...                           ...              ...         ...        ...   \n",
       "47518           Front-Wheel Drive     Compact Cars       (FFS)  13.523182   \n",
       "47519           Front-Wheel Drive     Compact Cars       (FFS)  12.935217   \n",
       "47520  4-Wheel or All-Wheel Drive     Compact Cars       (FFS)  14.167143   \n",
       "47521  4-Wheel or All-Wheel Drive     Compact Cars       (FFS)  14.167143   \n",
       "47522  4-Wheel or All-Wheel Drive     Compact Cars  (FFS,TRBO)  16.528333   \n",
       "\n",
       "       city08  comb08  ...  evMotor cylinders  displ  fuelCost08  fuelType  \\\n",
       "0          19      21  ...      NaN       4.0    2.0        2450   Regular   \n",
       "1           9      11  ...      NaN      12.0    4.9        4700   Regular   \n",
       "2          23      27  ...      NaN       4.0    2.2        1900   Regular   \n",
       "3          10      11  ...      NaN       8.0    5.2        4700   Regular   \n",
       "4          17      19  ...      NaN       4.0    2.2        3400   Premium   \n",
       "...       ...     ...  ...      ...       ...    ...         ...       ...   \n",
       "47518      19      22  ...      NaN       4.0    2.2        2350   Regular   \n",
       "47519      20      23  ...      NaN       4.0    2.2        2250   Regular   \n",
       "47520      18      21  ...      NaN       4.0    2.2        2450   Regular   \n",
       "47521      18      21  ...      NaN       4.0    2.2        2450   Regular   \n",
       "47522      16      18  ...      NaN       4.0    2.2        3600   Premium   \n",
       "\n",
       "      highway08  trans_dscr                 createdOn offset  \\\n",
       "0            25         NaN 2013-01-01 00:00:00-05:00    EST   \n",
       "1            14         NaN 2013-01-01 00:00:00-05:00    EST   \n",
       "2            33         SIL 2013-01-01 00:00:00-05:00    EST   \n",
       "3            12         NaN 2013-01-01 00:00:00-05:00    EST   \n",
       "4            23         NaN 2013-01-01 00:00:00-05:00    EST   \n",
       "...         ...         ...                       ...    ...   \n",
       "47518        26       CLKUP 2013-01-01 00:00:00-05:00    EST   \n",
       "47519        28         NaN 2013-01-01 00:00:00-05:00    EST   \n",
       "47520        24       CLKUP 2013-01-01 00:00:00-05:00    EST   \n",
       "47521        24         NaN 2013-01-01 00:00:00-05:00    EST   \n",
       "47522        21       CLKUP 2013-01-01 00:00:00-05:00    EST   \n",
       "\n",
       "                   str_date  \n",
       "0      Jan 01 00:00:00 2013  \n",
       "1      Jan 01 00:00:00 2013  \n",
       "2      Jan 01 00:00:00 2013  \n",
       "3      Jan 01 00:00:00 2013  \n",
       "4      Jan 01 00:00:00 2013  \n",
       "...                     ...  \n",
       "47518  Jan 01 00:00:00 2013  \n",
       "47519  Jan 01 00:00:00 2013  \n",
       "47520  Jan 01 00:00:00 2013  \n",
       "47521  Jan 01 00:00:00 2013  \n",
       "47522  Jan 01 00:00:00 2013  \n",
       "\n",
       "[47523 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(autos\n",
    " .assign(cylinders=autos.cylinders.fillna(0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sklearn pipeline to fill in missing cylinders with 0\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# set output to pandas\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n",
    "\n",
    "# create pipeline for cylinders\n",
    "cyl_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "])\n",
    "\n",
    "# try it out\n",
    "cyl_pipe.fit_transform(autos[['cylinders']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see where it fill in missing values\n",
    "(cyl_pipe\n",
    " .fit_transform(autos[['cylinders']])\n",
    " .loc[autos.cylinders.isna()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create more realistic pipeline\n",
    "# set missing cylinders to 0 and displ to median\n",
    "from sklearn.compose import ColumnTransformer\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Fit and transform the data\n",
    "pipeline.fit_transform(autos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning\n",
    "\n",
    "*Binning* is a process of transforming continuous numerical variables into discrete categorical 'bins'. Binning is used for:\n",
    "\n",
    "- Converting a continuous feature to a categorical feature\n",
    "- Helping with non-linear relationships\n",
    "- Reducing the effects of noise and outliers\n",
    "- Handling missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        19\n",
       "1         9\n",
       "2        23\n",
       "3        10\n",
       "4        17\n",
       "         ..\n",
       "47518    19\n",
       "47519    20\n",
       "47520    18\n",
       "47521    18\n",
       "47522    16\n",
       "Name: city08, Length: 47523, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autos.city08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a histogram of city08\n",
    "\n",
    "autos.city08.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a probability plot to see if it is normally distributed\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "stats.probplot(autos.city08, plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(autos.query('city08 < 40').city08, plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin the city08 data with pandas\n",
    "pd.cut(autos.city08, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.cut(autos.city08, bins=10)\n",
    " .value_counts()\n",
    " .sort_index()\n",
    " .plot.bar()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually create edges for bins\n",
    "(pd.cut(autos.city08, bins=[5,10,15,20,40, 140])\n",
    " .value_counts()\n",
    " .sort_index()\n",
    " .plot.bar()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning with sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Define the binning strategy\n",
    "binning_strategy = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('binning', binning_strategy, ['city08'])\n",
    "    ],\n",
    "    remainder='passthrough'  # This ensures other columns are left unchanged\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[('transformer', column_transformer)])\n",
    "pipeline.fit_transform(autos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit_transform(autos).binning__city08.value_counts().sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "binning_strategy = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('binning', binning_strategy, ['city08'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Fit and transform the data\n",
    "pipeline.fit_transform(autos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace pyarrow numbers with numpy numbers\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "binning_strategy = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('binning', binning_strategy, ['city08'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Fit and transform the data\n",
    "pipeline.fit_transform(autos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transform\n",
    "\n",
    "Log transformation is a data transformation method in which it replaces each variable x with a log(x). This is useful when the data is skewed and you are using a model that assumes normality or linearity.\n",
    "\n",
    "It is also common to use log transformation on the target variable y in regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot to show log of city08 vs original\n",
    "import numpy as np\n",
    "(autos\n",
    "    .assign(city08_log=np.log(autos.city08))\n",
    "    .plot.scatter(x='city08_log', y='city08')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos.city08.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot to show log of city08 vs original\n",
    "\n",
    "(autos\n",
    "    .assign(city08_log=np.log(autos.city08))\n",
    "    .city08_log.hist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model with linear regression to predict city08\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08']).select_dtypes('number')\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, pipeline.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again with log transform of y\n",
    "# create X and y\n",
    "X = X\n",
    "y_log = np.log(autos.city08)\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train_log, y_test_log = train_test_split(X, y_log, random_state=42)\n",
    "\n",
    "# pipeline\n",
    "pipeline_log = Pipeline(steps=[('preprocessor', preprocessor), ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline_log.fit(X_train, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_log.score(X_test, y_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take exp of predictions and score the mean absolute error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_pred_log = np.exp(pipeline_log.predict(X_test))\n",
    "\n",
    "mean_absolute_error(np.exp(y_test_log), y_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "*Scaling* is an ambiguous term that generally means one of two things:\n",
    "\n",
    "-  Min-max scaling - Changing the range of a variable to be between 0 and 1 or -1 to 1.\n",
    "-  Standard scaling (Standardization) - Changing the distribution of a variable to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "We'll show examples of both below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08']).select_dtypes('number')\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                           ('std_scaler', std_scaler),\n",
    "                           #('minmax_scaler', minmax_scaler), \n",
    "                           ('lr', LinearRegression())])\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Basic Techniques\n",
    "\n",
    "Predicting mileage from *barrels08*.\n",
    "\n",
    "- Make a model to predict *city08* from *barrels08* using linear regression.\n",
    "- What is the score?\n",
    "- Scatter plot *barrel08* vs *city08*\n",
    "- Make a new model transforming *barrels08* based on the results of the scatter plot.\n",
    "- How does the new model perform?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Basic Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "Many ML algorithms cannot work with categorical data directly. The categories must be converted into numbers. This process is called *encoding*. \n",
    "\n",
    "One of the most common encodings is *one hot encoding*, also called *dummy encoding*. This creates a new column for each category with a 1 or 0 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos.VClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(autos.VClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(autos.VClass, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cardinality - number of unique values in a column\n",
    "# probably don't want to make ~5k model columns\n",
    "(autos\n",
    " .select_dtypes(object) # use 'string[pyarrow]' if using pyarrow types\n",
    " .nunique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cardinality - number of unique values in a column\n",
    "# probably don't want to make ~5k model columns\n",
    "(autos\n",
    " .select_dtypes(object)\n",
    " .nunique()\n",
    " .index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical encoding in pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10)\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "\n",
    "        ('one_hot_encoder', one_hot_encoder, ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ],),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                          ('std_scaler', std_scaler),\n",
    "                           #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "                           ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values AND convert Pandas 2 strings to Pandas 1 strings\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False)\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('cat_imputer', cat_imputer, cat_cols),\n",
    "        ('one_hot_encoder', one_hot_encoder, cat_cols),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                          ('std_scaler', std_scaler),\n",
    "                           #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "                           ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train.assign(**X_train.select_dtypes('string[pyarrow]').astype(str)), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug with FunctionTransformer\n",
    "# And figure out that I need a separate pipeline for categorical columns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('cat_imputer', cat_imputer),\n",
    "    ('one_hot_encoder', one_hot_encoder)\n",
    "])\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('cat_pl', cat_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                           ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "                           ('std_scaler', std_scaler),\n",
    "                           #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "                           ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with missing categories in test set w/ handle_unknown='ignore'\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Add handle_unknown='ignore' to OneHotEncoder\n",
    "# one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False)\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('cat_imputer', cat_imputer),\n",
    "    ('one_hot_encoder', one_hot_encoder)\n",
    "])\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('cat_pl', cat_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                           ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "                          ('std_scaler', std_scaler),\n",
    "                           #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "                           ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Encoding\n",
    "\n",
    "*Hash encoding* is a technique that can be used when there are too many categories to encode with one hot encoding. It is similar to one hot encoding, but the categories are hashed into a smaller number of columns.\n",
    "\n",
    "We are going to use the `category_encoders` library to do the encoding. This library has many other encoders that you can explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(autos\n",
    " .select_dtypes(object)\n",
    " .nunique()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(autos\n",
    " .select_dtypes(object)\n",
    " .nunique()\n",
    " .pipe(lambda s: s[s > 40])\n",
    " .index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low cardinality columns\n",
    "(autos\n",
    " .select_dtypes(object)\n",
    " .nunique()\n",
    " .index\n",
    " .difference(high_cardinality_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace one hot encoder with hashing encoder for high cardinality columns\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# add hashing encoder\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        #('cat_pl', cat_pipe, cat_cols),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        ('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                           ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "                          ('std_scaler', std_scaler),\n",
    "                           #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "                           ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Encoding\n",
    "\n",
    "From docstring: [In Target Encoding] Each category is encoded based on a shrunk estimate of the average target\n",
    "values for observations belonging to the category. The encoding scheme mixes\n",
    "the global target mean with the target mean conditioned on the value of the\n",
    "category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import TargetEncoder\n",
    "\n",
    "te = TargetEncoder(target_type='continuous', random_state=42)\n",
    "te.fit_transform(X_train[['make']], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                           ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "                          ('std_scaler', std_scaler),\n",
    "                           #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "                           ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = autos[['barrels08']]\n",
    "y = autos['city08']\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[('lr', LinearRegression())])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test.assign(**X_test.select_dtypes('string[pyarrow]').astype(str)),\n",
    "                y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a custom class transformer to remove PyArrow strings (if using PyArrow)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class PyArrowStringConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.assign(**X.select_dtypes('string[pyarrow]').astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "    ('std_scaler', std_scaler),\n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Challenge\n",
    "\n",
    "Create a model to predict mileage using only the categorical columns (dropping the *model* column)\n",
    "\n",
    "```\n",
    "cat_cols = ['trany', 'drive', 'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Categorical Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction    \n",
    "\n",
    "Feature extraction is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "*Principal Component Analysis* (PCA) is a technique for reducing the dimensionality of data. It can also remove noise and might be useful as feature engineering for other algorithms. See my ML algorigthms course for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "# import pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "class PyArrowStringConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.assign(**X.select_dtypes('string[pyarrow]').astype(str))\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('std_scaler', std_scaler),\n",
    "    ('pca', PCA(n_components=10)),\n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),    \n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pros - Noise removed, less columns\n",
    "# Cons - Less explainable\n",
    "tmp_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Aggregation\n",
    "\n",
    "Use grouping to create new features. For example, we can group by manufacturer and then calculate the average fuel economy for each manufacturer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformer to add mean and std for y for a given column in X\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class AddMeanStd(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col, missing_mean_val, missing_std_val):\n",
    "        self.col = col\n",
    "        # attribute names must be the same as the constructor args\n",
    "        self.missing_mean_val = missing_mean_val\n",
    "        self.missing_std_val = missing_std_val\n",
    "        # track values for each group in col\n",
    "        self.means = {}\n",
    "        self.stds = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        assert y.name not in X.columns\n",
    "        with_y = X.assign(y=y)\n",
    "        self.means = with_y.groupby(self.col)['y'].mean().to_dict()\n",
    "        self.stds = with_y.groupby(self.col)['y'].std().to_dict()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # add mean and std for each row\n",
    "        return X.assign(**{\n",
    "            f'{self.col}_mean': X[self.col].map(self.means).fillna(self.missing_mean_val),\n",
    "            f'{self.col}_std': X[self.col].map(self.stds).fillna(self.missing_std_val)\n",
    "        })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline wtih Aggregation\n",
    "from sklearn.decomposition import PCA\n",
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('make_mean_std', AddMeanStd(col='make', missing_mean_val=0, missing_std_val=0)),\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('std_scaler', std_scaler),\n",
    "    #('pca', PCA(n_components=10)),\n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),    \n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF\n",
    "\n",
    "*Term frequency–inverse document frequency* (TFIDF) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a feature for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def combine_str_cols_transformer(X, cols, new_col_name):\n",
    "    # tdidf expects a single column of strings\n",
    "    return X.assign(**{new_col_name: X[cols].fillna('').agg(' '.join, axis='columns')})[new_col_name]\n",
    "\n",
    "text_pipeline = Pipeline([\n",
    "    ('combine_str', FunctionTransformer(combine_str_cols_transformer, \n",
    "                                        kw_args={'cols': cat_cols, 'new_col_name': 'all_str'})),\n",
    "    ('tfidf', TfidfVectorizer()), # can't be sparse because of Pandas\n",
    "    ('combine_debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_combine'})),\n",
    "    ('make_dense', FunctionTransformer(lambda X: X.toarray())),\n",
    "    ('pca', PCA(n_components=10)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline.fit_transform(autos[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_combine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tmp_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add TFIDF to combination of string columns\n",
    "from sklearn.decomposition import PCA\n",
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "def combine_str_cols_transformer(X, cols, new_col_name):\n",
    "    # tdidf expects a single column of strings\n",
    "    return X.assign(**{new_col_name: X[cols].fillna('').agg(' '.join, axis='columns')})[new_col_name]\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "\n",
    "text_pipeline = Pipeline([\n",
    "    ('combine_str', FunctionTransformer(combine_str_cols_transformer, \n",
    "                                        kw_args={'cols': cat_cols, 'new_col_name': 'all_str'})),\n",
    "    ('tfidf', TfidfVectorizer()), # can't be sparse because of Pandas\n",
    "    ('combine_debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_combine'})),\n",
    "    ('make_dense', FunctionTransformer(lambda X: X.toarray())),\n",
    "    ('pca', PCA(n_components=10)),\n",
    "])\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols),\n",
    "        ('text', text_pipeline, cat_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('make_mean_std', AddMeanStd(col='make', missing_mean_val=0, missing_std_val=0)),\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('std_scaler', std_scaler),\n",
    "    #('pca', PCA(n_components=10)),\n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),    \n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text pipeline to sklearn transformer so we can keep in pandas\n",
    "# and preserve index\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "def combine_str_cols_transformer(X, cols, new_col_name):\n",
    "    # tdidf expects a single column of strings\n",
    "    return X.assign(**{new_col_name: X[cols].fillna('').agg(' '.join, axis='columns')})[new_col_name]\n",
    "\n",
    "\n",
    "class TextPipeline(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cat_cols):\n",
    "        self.cat_cols = cat_cols\n",
    "        self.text_pipeline = Pipeline([\n",
    "            ('combine_str', FunctionTransformer(combine_str_cols_transformer, \n",
    "                                                kw_args={'cols': cat_cols, 'new_col_name': 'all_str'})),\n",
    "            ('tfidf', TfidfVectorizer()), # can't be sparse because of Pandas\n",
    "            ('make_dense', FunctionTransformer(lambda X: X.toarray())),\n",
    "            ('pca', PCA(n_components=10)),\n",
    "        ])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.text_pipeline.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        res = self.text_pipeline.transform(X)\n",
    "        # replace index with X index\n",
    "        df = (res\n",
    "              .assign(index=X.index)\n",
    "              .set_index('index')\n",
    "        )\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the pipeline\n",
    "text_pipeline = TextPipeline(cat_cols)\n",
    "text_pipeline.fit(X_train, y_train)\n",
    "text_pipeline.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add TFIDF to combination of string columns\n",
    "from sklearn.decomposition import PCA\n",
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols),\n",
    "        ('text', TextPipeline(cat_cols), cat_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('make_mean_std', AddMeanStd(col='make', missing_mean_val=0, missing_std_val=0)),\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),        \n",
    "    ('std_scaler', std_scaler),\n",
    "    #('pca', PCA(n_components=10)),\n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embeddings\n",
    "\n",
    "*Text embeddings* are a type of feature extraction that is used for text data. They are a numerical representation of text that can be used in machine learning algorithms. They are often used as a feature for text classification. A common example is a vector to represent man, woman, and king. When you add the difference between woman and man to king, you get queen.\n",
    "\n",
    "We will use the Spacy library to create text embeddings. Spacy is a library for natural language processing (NLP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy language model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class SpacyEmbeddingVectorizer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, columns):\n",
    "        # Load the SpaCy model\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        text = (X[self.columns].fillna('').apply(lambda x: ' '.join(x), axis='columns'))\n",
    "        res = [self.nlp(row).vector for row in text]\n",
    "        df = pd.DataFrame(res, index=X.index)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it out on cat_cols - takes 4+ minutes on my machine\n",
    "# using sample to speed up\n",
    "\n",
    "embeds = SpacyEmbeddingVectorizer(cat_cols)\n",
    "embeds.fit_transform(X_train.sample(1_000, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "def combine_str_cols_transformer(X, cols, new_col_name):\n",
    "    # tdidf expects a single column of strings\n",
    "    return X.assign(**{new_col_name: X[cols].agg(' '.join, axis='columns')})[new_col_name]\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols),\n",
    "        ('text', SpacyEmbeddingVectorizer(cat_cols), cat_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('make_mean_std', AddMeanStd(col='make', missing_mean_val=0, missing_std_val=0)),\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),        \n",
    "    ('std_scaler', std_scaler),\n",
    "    #('pca', PCA(n_components=10)),\n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "limit = 1_000\n",
    "pipeline.fit(X_train.iloc[:limit], y_train.iloc[:limit])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Feature Extraction\n",
    "\n",
    "Create a model that predicts mileage based on the spacy embeddings of the text columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "column_transformers = ColumnTransformer(\n",
    "        transformers=[\n",
    "                ('text', SpacyEmbeddingVectorizer(cat_cols), cat_cols)\n",
    "        ],\n",
    "        remainder='drop' # drop everything else\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "  ('preprocessor', column_transformers),\n",
    "  ('LinearRegression', LinearRegression())\n",
    "])\n",
    "\n",
    "# fit the pipeline\n",
    "limit = 1_000\n",
    "pipeline.fit(X_train.iloc[:limit], y_train.iloc[:limit])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Features\n",
    "\n",
    "Time based data often has trends and seasonality. We can extract features from the date and time to capture these patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Date and Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install feature-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use feature-engine library to pull out date features\n",
    "\n",
    "from feature_engine.datetime import DatetimeFeatures\n",
    "\n",
    "dtf = DatetimeFeatures(features_to_extract='all')\n",
    "dtf.fit_transform(autos[['createdOn']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality and Trend\n",
    "\n",
    "We can use the `seasonal_decompose` function from `statsmodels`` to decompose a time series into its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "seasonal_decompose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ford = (autos\n",
    "        .query(\"make == 'Ford'\")\n",
    "        #.groupby(pd.Grouper(key='createdOn', freq='ME'))\n",
    "        .groupby('year')\n",
    "        .city08\n",
    "        .median()\n",
    "        .ffill())\n",
    "ford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of 5 year decomposition (period=5 on yearly data)\n",
    "from matplotlib import pyplot as plt\n",
    "decomposition = seasonal_decompose(ford, model='additive', period=5)\n",
    "\n",
    "# Plot the decomposition\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(15, 8))\n",
    "ford.plot(ax=ax1, title='Original')\n",
    "decomposition.trend.plot(ax=ax2, title='Trend')\n",
    "decomposition.seasonal.plot(ax=ax3, title='Seasonality')\n",
    "decomposition.resid.plot(ax=ax4, title='Residuals')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "def get_seasonal(group, time_col, agg_col, period=1):\n",
    "    # Sort by date and set index\n",
    "    group = group.sort_values(time_col)\n",
    "    # rename index to index\n",
    "    group.index.name = 'index'\n",
    "    ts = group.set_index(time_col)[agg_col]\n",
    "\n",
    "    # Handle groups with insufficient data\n",
    "    if len(ts) < 2:\n",
    "        return group.assign(seasonal=0, trend=0, resid=0)\n",
    "\n",
    "    # Decompose the time series\n",
    "    res = seasonal_decompose(ts, model='additive', period=period, extrapolate_trend='freq')\n",
    "\n",
    "    # Reassign the decomposed components to the group\n",
    "    return (group\n",
    "            .assign(seasonal=res.seasonal.values,\n",
    "                    trend=res.trend.values,\n",
    "                    resid=res.resid.values))\n",
    "\n",
    "def add_seasonal(df, time_col, group_cols, agg_col):\n",
    "    all_group_cols = [time_col] + group_cols\n",
    "    return (df\n",
    "            .groupby(group_cols)\n",
    "            .apply(get_seasonal, time_col=time_col, agg_col=agg_col, period=1)\n",
    "            .drop(columns=group_cols)\n",
    "            .reset_index(drop=False)\n",
    "            .set_index('index')\n",
    "            .loc[df.index]\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "res = add_seasonal(X, 'year', ['make'], agg_col='barrels08')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.iloc[:, -5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline wtih Aggregation\n",
    "from sklearn.decomposition import PCA\n",
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "# create the seasonal transformer\n",
    "class SeasonTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, time_col, group_cols, agg_col):\n",
    "        self.time_col = time_col\n",
    "        self.group_cols = group_cols\n",
    "        self.agg_col = agg_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return add_seasonal(X, self.time_col, self.group_cols, self.agg_col)\n",
    "        \n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('make_mean_std', AddMeanStd(col='make', missing_mean_val=0, missing_std_val=0)),\n",
    "        ('seasonal_decompose', SeasonTransformer(time_col='year', group_cols=['make'], agg_col='barrels08')),\n",
    "#          ['year', 'make', \n",
    "#           'barrels08']),  # need to make sure we pass all columns needed\n",
    "\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),    \n",
    "    ('std_scaler', std_scaler),\n",
    "    #('pca', PCA(n_components=10)),\n",
    "\n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model to predict monthly airline passgengers from https://www.transtats.bts.gov/Data_Elements.aspx?Data=1\n",
    "# for some reason this output is not Excel but HTML\n",
    "airlines = pd.read_html('data/Passengers_2024.xls')\n",
    "raw = airlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweak_airline(df_):\n",
    "  return (df_\n",
    "          .query('Month != \"TOTAL\"')\n",
    "          .astype({'Month': 'int64'})\n",
    "          .assign(date=lambda df: pd.to_datetime(df[['Year', 'Month']].assign(day=1)))\n",
    "  )\n",
    "\n",
    "air = tweak_airline(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Temporal Features\n",
    "\n",
    "Using the airline data, create a linear regression model to predict the number of passengers for the next month based on the current month.\n",
    "\n",
    "Then make another model using the `seasonal_decompose` function to add a seasonality component to the model. How do the models compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "*Feature selection* is the process of selecting a subset of features that are most relevant to the target variable. There are many reasons to do this:\n",
    "\n",
    "-  Simplicity - Fewer features are easier to interpret.\n",
    "-  Memory - Fewer features require less memory to store and less computation.\n",
    "-  Speed - Fewer features result in faster algorithms.\n",
    "\n",
    "There are many techniques for feature selection. We will cover a few of them here:\n",
    "\n",
    "-  Feature importance\n",
    "-  Recursive feature elimination\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance and Weights\n",
    "\n",
    "Many models can provide a feature importance or weight for each feature. This is a measure of how much the model depends on that feature. We can use this to select the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline wtih Aggregation\n",
    "from sklearn.decomposition import PCA\n",
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "# create the seasonal transformer\n",
    "class SeasonTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, time_col, group_cols, agg_col):\n",
    "        self.time_col = time_col\n",
    "        self.group_cols = group_cols\n",
    "        self.agg_col = agg_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return add_seasonal(X, self.time_col, self.group_cols, self.agg_col)\n",
    "        \n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('make_mean_std', AddMeanStd(col='make', missing_mean_val=0, missing_std_val=0)),\n",
    "        ('seasonal_decompose', SeasonTransformer(time_col='year', group_cols=['make'], agg_col='barrels08')),\n",
    "#          ['year', 'make', \n",
    "#           'barrels08']),  # need to make sure we pass all columns needed\n",
    "\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),    \n",
    "    ('std_scaler', std_scaler),\n",
    "    #('pca', PCA(n_components=10)),\n",
    "\n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get linear regression model\n",
    "\n",
    "lr = pipeline.named_steps['lr']\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(lr.coef_, index=lr.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(lr.coef_, index=lr.feature_names_in_)\n",
    " .sort_values(key=abs)\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lr.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(lr.coef_, index=lr.feature_names_in_)\n",
    " .sort_values(key=abs)\n",
    " .tail(25)\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note scale of x axis\n",
    "(pd.Series(lr.coef_, index=lr.feature_names_in_)\n",
    " .sort_values(key=abs)\n",
    " .head(26)\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use xgboost to make a model and then get feature importances\n",
    "import xgboost as xgb\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.assign(**X.select_dtypes(object).astype('category')),\n",
    "    y, random_state=42)\n",
    "\n",
    "xg = xgb.XGBRegressor(enable_categorical=True, random_state=42)\n",
    "xg.fit(X_train, y_train)\n",
    "xg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(xg.feature_importances_, index=X_train.columns).sort_values().tail(25).plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot range vs city mpg\n",
    "(autos\n",
    " .plot.scatter(x='range', y='city08')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(xg.feature_importances_, index=X_train.columns).sort_values().iloc[:-1].plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make xg model with just range, barrels08, fuelType, and cylinders\n",
    "xg_simple = xgb.XGBRegressor(enable_categorical=True, random_state=42)\n",
    "xg_simple.fit(X_train[['range', 'barrels08', 'fuelType', 'cylinders']], y_train)\n",
    "xg_simple.score(X_test[['range', 'barrels08', 'fuelType', 'cylinders']], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "*Recursive feature elimination* (RFE) is a feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached. It is a greedy optimization algorithm that aims to find the best performing feature subset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# use rfe with xgboost\n",
    "xg_model = xgb.XGBRegressor(enable_categorical=True, random_state=42)\n",
    "rfe = RFE(xg_model, n_features_to_select=3)\n",
    "\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #X.assign(**X.select_dtypes('string[pyarrow]').astype('category')),\n",
    "    X.select_dtypes('number'),\n",
    "    y, random_state=42)\n",
    "\n",
    "rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the API to determine which features were selected\n",
    "rfe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'features':X_train.columns,\n",
    "              'support': rfe.support_,\n",
    "              'ranking':rfe.ranking_}).sort_values('ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model with barrels08, range, and fuelCost08\n",
    "xg_simple = xgb.XGBRegressor(enable_categorical=True, random_state=42)\n",
    "xg_simple.fit(X_train[['barrels08', 'range', 'fuelCost08']], y_train)\n",
    "xg_simple.score(X_test[['barrels08', 'range', 'fuelCost08']], y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Random Column\n",
    "\n",
    "Another technique is to add a column of random numbers. We should be able to drop any columns that perform worse than the random column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and add a random column\n",
    "X = (autos\n",
    "     .drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "     .assign(random=lambda df: np.random.random(size=len(df)))\n",
    "     )\n",
    "\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.assign(**X.select_dtypes(object).astype('category')),\n",
    "    y, random_state=42)\n",
    "\n",
    "xg = xgb.XGBRegressor(enable_categorical=True, random_state=42)\n",
    "xg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at feature importances\n",
    "# In this case the random column is the least important \n",
    "pd.Series(xg.feature_importances_, index=X_train.columns).sort_values().plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Feature Selection\n",
    "\n",
    "Apply RFE to linear regression model to limit model to 5 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion - Next Steps\n",
    "\n",
    "- Practice! - Watching and listening is not enough. You need to practice what you have learned.\n",
    "- Understand your data - You need to understand your data and the problem you are trying to solve.\n",
    "- Master Pandas and Scikit-Learn - These are the most important tools for feature engineering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
